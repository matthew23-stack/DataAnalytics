# DataAnalytics

# Module 1: The Basics of Data
What is Data Analytics?🙌🙌
The role of a data analyst is to transform raw data into actionable insights that guide decision-making processes within an organization.involves several key responsibilities and skills.Data Collection and Preparation:Sourcing data from various channels, including databases, spreadsheets, and external sources,Data Analysis:Employing statistical methods, machine learning techniques, or other analytic tools to interpret data,Data Visualization and Storytelling:Creating visual representations of the data, such as charts, graphs, and dashboards, to make complex information easily understandable,Decision Support:Making recommendations based on data-driven insights to help guide business decisions,Continuous Learning and Adaptation:Keeping up-to-date with the latest industry trends, tools, and technologies in data analysis.A data analyst helps an organization make informed decisions that can improve operational efficiencies, drive business strategies, and create a competitive advantage.Professionals who possess the skills and knowledge required to perform this vital work.They understand how the organization can acquire, clean, and transform data to meet the organization's needs.
Data ✨✨
The amount of data the modern world generates on a daily basis is staggering. From the organized tables of spreadsheets to the storage of photos, video, and audio recordings, modern businesses create an almost overwhelming avalanche of data that is ripe for use in analytics programs.
Storage🎂🎂
The second key trend driving the growth of analytics programs is the increased availability of storage at rapidly decreasing costs.
Computing Power✔✔
Today, the effects of Moore's Law have democratized computing. Most employees in an organization now have enough computing power sitting on their desks to perform a wide variety of analytic tasks. If they require more powerful computing resources, cloud services allow them to rent massive banks of computers at very low cost. Even better, those resources are charged at hourly rates, and analysts pay only for the computing time that they actually use.
Career in Analytics🎁🎁
As businesses try to keep up with these trends, hiring managers find themselves struggling to identify, recruit, and retain talented analytics professionals.Data analysts and scientists,AI and machine learning (ML) specialists,Big Data specialists,Digital marketing and strategy specialists,Process automation specialists,Business development professionals,Digital transformation specialists,Information security analyst,Software and applications developers,Internet of Things (IoT) specialists.
The Analytics Process👌👌
Analysts working with data move through a series of different steps as they seek to gain business value from their organization's data.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/a84e8897-36d0-4f34-9159-7e6dc4baccb7)
The Analytics Process is Iterative
While we describe the steps of the analytics process as a series of sequential actions, it is more accurate to think of them as a set of interrelated actions that may be revisited frequently while working with a dataset.
Analytics Techniques🐱‍🏍🐱‍🏍
Analysts use a variety of techniques to draw conclusions from the data at their disposal.help you understand the purpose of different types of analysis, we often group these techniques into categories based on the purpose of the analysis and/or the nature of the tool.
Machine Learning, Artificial Intelligence, and Deep Learning🐱‍🚀🐱‍🚀
Machine learning uses algorithms to discover knowledge in your datasets that you can then apply to help you make informed decisions about the future. here are some cases where machine learning commonly adds value:Segmenting customers and determining the marketing messages that will appeal to different customer groups.Discovering anomalies in system and application logs that may be indicative of a cybersecurity incident.Forecasting product sales based on market and environmental conditions.Machine learning can bring value to almost every field where discovering previously unknown knowledge is useful.As we move through the world, we hear the terms artificial intelligence, machine learning, and deep learning being used almost interchangeably to describe any sort of technique where computers are working with data.Artificial intelligence (AI) includes any type of technique where you are attempting to get a computer system to imitate human behavior. As the name implies, you are trying to ask computer systems to artificially behave as if they were intelligent.Machine learning (ML) is a subset of AI techniques. ML techniques attempt to apply statistics to data problems in an effort to discover new knowledge. Or, in other terms, ML techniques are AI techniques designed to learn.Deep learning is a further subdivision of machine learning that uses quite complex techniques, known as neural networks, to discover knowledge in a particular way. It is a highly specialized subfield of machine learning that is most commonly used for image, video, and sound analysis.
Data Governance🐱‍🐉🐱‍🐉
Notice that there is a slab of stone that supports the three pillars of analytics. This slab represents the important role of data governance in analytics programs. Without strong governance, analytics programs cannot function effectively.Data governance programs ensure that the organization has high-quality data and is able to effectively control that data.how organizations use master data management (MDM) programs to maintain and improve the quality of their data.
Analytics Tools🐱‍💻🐱‍💻
hese tools automate much of the heavy lifting of data analysis, improving the analyst's ability to acquire, clean, manipulate, visualize, and analyze data.Some of these tools are well-known to most computer users. For example, people are generally familiar with spreadsheet tools such as Microsoft Excel or Google Sheets.
Chapter 1 Summary 🙌
Analytics programs allow businesses to access the untapped value locked within their data. Today, many organizations recognize the potential value of this work but are still in the early stages of developing their analytics programs. These programs, driven by the unprecedented availability of data, the rapidly decreasing cost of storage, and the maturation of cloud computing, promise to create significant opportunities for businesses and, in turn, for data professionals skilled in the tools and techniques of analytics.As analysts develop analytic work products, they generally move through a series of stages. Their work begins with the acquisition of data from internal and external sources and continues with the cleaning and manipulation of that data. Once data is in a suitable form, data professionals apply analytic techniques to draw conclusions from their data, create visualizations to depict the story of their data, and develop reports and dashboards to effectively communicate the results of their work to business leaders.

# Chapter 2 Understanding Data 
To understand data types, it is best first to understand data elements. A data element is an attribute about a person, place, or thing containing data within a range of values. Data elements also describe characteristics of activities, including orders, transactions, and events. Now that you understand what data elements are, let's explore how they relate to data types. A data type limits the values a data element can have.Pet Name, Animal Type, and Breed Name are all words. Meanwhile, the Date of Birth column contains numbers and slashes that identify a specific date. Height and Weight are both numbers. Each of these groupings represents a particular data type.
Tabular Data🐱‍👤🐱‍👤
Tabular data is data organized into a table, made up of columns and rows. A table represents information about a single topic. Each column represents a uniquely named field within a table, also called a variable, about a single characteristic. The contents of each column contain values for the data element as defined by the column header. The intersection of a row and column contains a specific value. Looking at the intersection of Jack and Breed Name in Table 2.1 tells us that Jack is a Corgi. If we want to identify Hazel's breed, we look at where her row intersects with the Breed Name column and see that she is a Labradoodle.Spreadsheets, including Microsoft Excel, Google Sheets, and Apple Numbers, are practical tools for representing tabular data. A relational database management system (RDMS), commonly called a database, extends the tabular model. Instead of having all data in a single table, a database organizes related data across multiple tables. The connection between tables is known as a relationship. Oracle, Microsoft SQL Server, MySQL, and PostgreSQL are examples of database software. Tabular data is the concept that underpins both spreadsheets and relational databases.
Structured Data Types🐬🐬
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/0e0d3780-0d06-45dd-8402-85c809c2d66d)
Structured data is tabular in nature and organized into rows and columns. Structured data is what typically comes to mind when looking at a spreadsheet. With clearly defined column headings, spreadsheets are easy to work with and understand. In a spreadsheet, cells are where columns and rows intersect.When you read the column headings, you get a good sense of the kind of data that you're going to find in that column. For example, when you see the “Weight (pounds)” column, you expect to see numeric values. In the Address field, you expect to see text. Looking more closely, we see that the data values are consistent for each column.
Character
The character data type limits data entry to only valid characters. Characters can include the alphabet that you might see on your keyboard, as well as numbers. Depending on your needs, multiple data types are available that can enforce character limits.Alphanumeric is the most widely used data type for storing character-based data. As the name implies, alphanumeric is appropriate when a data element consists of both numbers and letters.The alphanumeric data type is ideal for storing product stock-keeping units (SKUs). It is common in the retail clothing space to have a unique SKU for each item available for sale. If you sell jeans, you may stock products from Armani Jeans, Diesel, Lee Jeans, Levi's, and Wrangler. To keep track of all the manufacturer, size, color, and fit combinations in your inventory, you might use an SKU similar to the one depicted in Figure 2.2. Tracking inventory at the SKU level allows you to manage availability in your online and in-store systems, all courtesy of the alphanumeric data type.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/c9f59bee-7905-46ab-aa7b-d21a48791d21)
There are times when it is necessary to impose even stricter limits on character-related data to exclude numbers. Excluding numbers can be achieved using the text data type.Consider a data entry example. Suppose you operate an online retail system. To deliver orders, you need address information for the intended recipients. This information comes from the customers themselves since they can specify where orders should be shipped.
Character Sets😁😁
When considering alphanumeric and text data types, you need to think about the character set you are using to input and store data when using a database. Databases use character sets to map, or encode, data and store it digitally. The ASCII encoding standard is based on the U.S. English alphabet. ASCII accommodates both the upper and lowercase English alphabet and numbers, mathematical operators, and symbols.Many languages include accent marks, extending the Latin alphabet. For example, Akrapovič is a Slovenian manufacturer. To store the č in Akrapovič, you need to encode that value appropriately. In addition, there are many languages, including Arabic, Chinese, Japanese, and Korean, which use symbols as opposed to extending the Latin alphabet. For example, the Arabic word for “cat.” Several encoding standards exist that accommodate non-Latin characters.
Numeric
When numbers exclusively make up values for a data attribute, numeric becomes the data type of choice. This data type appears to be simple and obvious based on its name. As seen with the character data type, implementation nuances about numeric are essential to understand. Databases accommodate two types of numeric data types: integer and numeric.
Whole Numbers
The integer, and all its subtypes, are for storing whole numbers. As seen with the character family of data types, implementation differences exist across databases. Table 2.3 illustrates how Oracle, Microsoft, and MySQL support whole numbers. Note that both the Microsoft and MySQL databases support the bit data type, which can be empty or store a 0 or a 1. In computer science, flags indicate whether something is on or off, or if a function has completed successfully. To show something is on, 1 or TRUE is used. For a value of off, 0 or FALSE is used. The bit data type is intended for storing the status of a flag. Note also that the value ranges for smallint and shortinteger are identical. The same is true for int and integer, as well as bigint and longinteger. Although the data types have different names, their functionality is equivalent.
Rational Numbers
In all its variants, the numeric data type is for rational numbers that include a decimal point. As with the integer family of data types, each database vendor has its implementation nuances. Table 2.4 illustrates how Oracle, Microsoft, and MySQL support rational numbers.
Date and Time
Gathered together under the broad category of date, day of year and time of day are data elements that appear with great frequency. As illustrated in Table 2.5, databases have various data types for handling date- and time-related information. As seen with character and numeric data types, nuances exist across different databases. Selecting the appropriate date-related data type depends on the data you need to store.
Currency
Many people use spreadsheets to manage their finances. Organizations typically use enterprise-scale software for the same purpose, with the data residing in a database. While financial data is numeric, people prefer seeing the numbers displayed as a specific currency. For example, consider the Number, Dollar, and Euro.
Strong And Weak Typing
Data types define values placed in columns. Strong typing is when technology rigidly enforces data types. Databases, discussed in Chapter 3, use strong typing. A database column defined as numeric only accepts numerical values. You will get an error if you attempt to enter characters into a numeric column.Weak typing loosely enforces data types. Spreadsheets use weak typing to help make it easier for people to accomplish their work. Spreadsheets default to an “automatic” data type and accommodate practically any value. When a person specifies a data type, it is loosely enforced compared to a database. For example, with a numeric spreadsheet cell, the software does not stop you from entering and storing characters.Software that uses weak typing can be helpful. That said, be mindful that you may experience unexpected and perhaps incorrect results.
Unstructured Data Types😏😏
While much of the data we use to record transactions is highly structured, most of the world's data is unstructured. Unstructured data is any type of data that does not fit neatly into the tabular model.Examples of unstructured data include digital images, audio recordings, video recordings, and open-ended survey responses. Analyzing unstructured data creates a wealth of information and insight. Many people have camera-enabled smartphones, and using video for conversations and meetings is commonplace.
Binary
Binary data types are one of the most common data types for storing unstructured data. It supports any type of digital file you may have, from Microsoft Excel spreadsheets to digital photographs. When considering which binary data type to use, file size tends to be the limiting factor. You need to select a data type that is as large as the largest file you plan on storing.
Audio
Audio data can come from a variety of sources. Whenever you interact with a customer service agent and hear “this call may be recorded for quality assurance purposes,” your conversation is probably being recorded and stored for later analysis. The impact of capturing, storing, and analyzing audio data has led to the development of avalanche detection systems. These systems listen for and detect the acoustic characteristics of an avalanche. With real-time notification capabilities, these systems reduce the time it takes for emergency services to respond and alert hikers to treacherous conditions.
Images
Image data can come from a variety of sources. People take more than 1 trillion photographs every calendar year, fuelled by the ubiquity of camera-enabled smartphones and relatively low storage costs. Each digital picture is a piece of unstructured data. 
Video
Video data is growing at a similar pace to image data. In the consumer space, people upload videos to YouTube, Instagram, and TikTok every day. Police officers wear body cameras to create a video record of enforcement situations. Image processing algorithms examine videos to detect everything from traffic congestion to intruders in the home.
Categories of Data😴😴
We try to fit data into structured and unstructured categories. The reality is that the world is not black and white, and not all data fits neatly into structured and unstructured categories. Semi-structured data represents the space between structured spreadsheets and unstructured videos.
Quantitative vs. Qualitative Data
Regardless of structure, data is either quantitative or qualitative. Quantitative data consists of numeric values. Data elements whose values come from counting or measuring are quantitative.
Discrete vs. Continuous Data
Numeric data comes in two different forms: discrete and continuous. A helpful way to think about discrete data is that it represents measurements that can't be subdivided. You may intuitively think of discrete data as using whole numbers, but that doesn't have to be the case. For example, if a fundraising organization sells chickens in half-chicken increments, you can buy 1.5 chickens. However, you can't buy .25 chickens.
Categorical Data
In addition to quantitative, numeric data, there is categorical data. Text data with a known, finite number of categories is categorical. When considering an individual data element, it is possible to determine whether or not it is categorical. Let's continue to identify each data element of the pet dataset in Table 2.1. Animal Type is a good example of categorical data. As represented, this column separates the data into two categories: dog and cat. As additional dogs or cats enter into care, they fall within the existing categories.
Dimensional Data
Dimensional modeling is an approach to arranging data to facilitate analysis. Dimensional modeling organizes data into fact tables and dimension tables. Fact tables store measurement data that is of interest to a business. A veterinary practice may want to answer some questions about appointments. A table holding appointment data would be called a fact table. Dimensions are tables that contain data about the fact.
Common Data Structures🤖🤖
In order to facilitate analysis, data needs to be stored in a consistent, organized manner. When considering structured data, several concepts and standards inform how to organize data. On the other hand, unstructured data has a wider variety of storage approaches
Structured Data💀💀
Tabular data is structured data, with values stored in a consistent, defined manner, organized into columns and rows. Data is consistent when all entries in a column contain the same type of value. This method of organization facilitates aggregation. For example, you can add each value in the Weight column in Table 2.1 to get the total weight for all animals.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/c6374470-6073-45a5-9c3b-4e756e9beed1)
Since both Pet Name and Animal Type are character data types, nothing from a structural standpoint prevents this mistake.Just as there is an expectation that the values in a given column are consistent, it is a convention that each row contains data about a single record.
Unstructured Data(●'◡'●)
Unstructured data is qualitative, describing the characteristics of an event or an object. Images, phrases, audio or video recordings, and descriptive text are all examples of unstructured data. There is very little that is common about different kinds of unstructured data. Since the data is highly variable, its organizational and storage needs are different from structured data. Unstructured data also represents a significant opportunity. A Forbes study shows that over 90 percent of businesses need to manage and derive value from unstructured data.A wide variety of technologies has emerged to facilitate the storage of unstructured data. Operationally, these technologies are similar to how a key in a tabular dataset identifies its associated values. With unstructured data, the key is a unique identifier, whereas the value is the unstructured data itself.Consider the log entry shown in Figure 2.22. As an example of machine data, it represents a single entry within a log file generated when accessing a specific image on the Internet. The log entry contains a mix of seemingly random strings, time stamps, IP addresses, URLs, and browser metadata.![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/73e66557-dcb7-4071-9bcf-1d9551fd24ef)
Semi-Structured Data😏😏
Semi-structured data is data that has structure and that is not tabular. Email is a well-known example of semi-structured data. Every email message has structural components, including recipient, sender, subject, date, and time. However, the body of an email is unstructured text, while attachments could be anything type of file.The need to make semi-structured data easier to work with has led to the emergence of semi-structured formatting options. These formatting options use separators or tags to provide some context around a data element. Let's explore common file formats for transporting semi-structured data.
Common File Formats🎈🎈
Common file formats facilitate data exchange and tool interoperability. Several file formats have emerged as standards and are widely adopted. As a modern data analyst, you will need to recognize all of these formats and be familiar with common use cases for each type.
Text Files🎆🎆
Text files are one of the most commonly used data file formats. As the name implies, they consist of plain text and are limited in scope to alphanumeric data. One of the reasons text files are so widely adopted is their ability to be opened regardless of platform or operating system without needing a proprietary piece of software.When machines generate data, the output is commonly stored in a text file. For example, the unstructured log entry. unique character known as a delimiter facilitates transmitting structured data via a text file. The delimiter is the character that separates individual fields. A delimiter can be any character. Over the years, the comma and tab grew into a widely accepted standard. Various software packages support reading and writing delimited files using the comma and the tab. In addition, many coding languages have libraries that make it easy to write comma- or tab-delimited files. When a file is comma-delimited, it is known as a comma-separated values (CSV) file. Similarly, when a file is tab-delimited, it is called a tab-separated values (TSV) file.You may think that all CSV files represent structured data. Consider Figure 2.27, containing an excerpt from playback-related events from a Netflix viewer. Every column header except for Playtraces is structured. However, note the contents of the Playtraces field within the red rectangle.
Fixed-Width Files🧨🧨
Before it was common to use delimited files with variable-length columns, flat files were fixed-width, as illustrated in Figure 2.28. Fixed-width files are more laborious to create since they require a few extra steps.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/b9d6e890-8b68-4f79-9e73-9073a15759df)
The first row in a fixed-width file describes the column names. For the data rows, you first need to determine the maximum length of each column. Then, you must pad values that are shorter than the maximum length. For numeric fields, you accomplish padding by prepending a leading zero. For alphanumeric or text fields, this is done by prepending or appending spaces.
JavaScript Object Notation✨✨
JavaScript Object Notation (JSON) is an open standard file format, designed to add structure to a text file without incurring significant overhead. One of its design principles is that JSON is easily readable by people and easily parsed by modern programming languages. Languages such as Python, R, and Go have libraries containing functions that facilitate reading and writing JSON files.To illustrate how a machine processes this same information, Figure 2.30 shows how the entire pet data, formatted as JSON, is read using the Python programming language. Figure 2.31 illustrates reading the same file using the R programming language. Note that R, which facilitates statistical analysis of data, has a summary command, the results of which illustrate some summary statistics about the pet data.
Extensible Markup Language (XML)🎄🎄
Extensible Markup Language (XML) is a markup language that facilitates structuring data in a text file. While conceptually similar to JSON, XML incurs more overhead because it makes extensive use of tags. Tags describe a data element and enclose each value for each data element. While these tags help readability, they add a significant amount of overhead.In 1999, XML was the data format of choice and facilitated Asynchronous JavaScript and XML (Ajax) web development techniques. AJAX allowed client applications, written in HTML, to retrieve data from a server asynchronously. Without having to wait for a server response, the speed with which dynamic web pages operated increased. With JSON as a lighter-weight alternative to XML, it is becoming increasingly popular when interacting asynchronously between a web browser and a remote server.
HyperText Markup Language (HTML)🎋🎋
HyperText Markup Language (HTML) is a markup language for documents designed to be displayed in a web browser. HTML pages serve as the foundation for how people interact with the World Wide Web. Similar to XML, HTML is a tag-based language. Figure 2.33 illustrates the creation of a table in HTML containing the data for a single pet. Figure 2.34 illustrates how a browser processes an HTML of fully populated pet data to display it to people.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/d2c28421-2451-4a2a-975a-70db983fae81)
Most people interact with HTML as interpreted by a web browser. HTML has become increasingly sophisticated over the years, with the ability for developers to create web pages that dynamically display content, adjust to different screen sizes, and play videos. Among the many tags that HTML supports is the image tag.
Chapter 2 Summary🎞🎞
When dealing with data, you need to think through the data values you are working with, because doing so influences your choice of data type. When using structured data, you may be working with dates, numbers, text, currency, or alphanumeric data. Whether the data is discrete, continuous, or categorical, choosing the appropriate data type can help boost data quality. There are also data types for storing unstructured data, such as images, audio, and video.If you are working with structured data, you should start thinking about it in a tabular fashion. Getting structured data into unique rows and consistent columns is the first step on the path to preparing data for analysis. Structured data fits well into CSV files, a popular format for exchanging data via flat files.When you have to incorporate additional metadata or represent a complex data structure, you need capabilities beyond what a flat-file provides. Formatting the data as JSON or XML is a viable alternative.
The modern analyst frequently works with data sources over the Internet. Understanding that HTML is the standard for structuring web pages is crucial to developing the ability to interact with data over the Internet programmatically.

# Chapter 3 Databases and Data Acquisition
learned how organizations rely on structured and unstructured data to meet their business needs. The vast majority of transactional systems generate structured data, and we need technology that will help us store all of that data. Databases are the foundational technology that allows us to keep our data organized and easy to retrieve when we need it.We collect data from many sources to derive insights. For example, a financial analyst wants to understand why some retail outlets are more profitable than others. The analyst might look at the individual records for each store, but that's difficult to do if there are thousands of items and hundreds of stores. In this chapter, you will examine the two main database categories: relational and nonrelational. You will learn several different approaches to designing databases to store structured data. Building on this foundation, you will learn about common database design practices. You will proceed to dive into the details of working with databases. You will examine tools and techniques that will help you answer questions using data.
The Relational Model
The relational model builds on the concept of tabular data. In the relational model, an entity contains data for a single subject. When creating an IT system, you need to consider all the entities required to make your system work. You can think of entities as nouns because they usually correspond to people, places, and things.Imagine a veterinary clinic that has to store data about the animals it treats and the people who own those animals. The clinic needs one entity for data about animals and a separate entity for data about people to accomplish this goal. Understanding that the header corresponds to the name of an entity, look at the rows of the Person entity. Each row represents an individual attribute associated with a person. Suppose you want to enhance the Person entity to accommodate a mobile phone number. You would just edit the Person entity to include a mobile phone attribute.Each of these entities becomes a separate table in the database, with a column for each attribute. Each row represents an instance of the entity. The power of the relational model is that it also allows us to describe how entities connect or relate, to each other. The veterinary clinic needs to associate animals with people. Animals do not have email addresses, cannot schedule appointments, and cannot pay for services. Their owners, on the other hand, do all of these things. The entity relationship diagram (ERD) is a visual artifact of the data modeling process. It shows the connection between related entities. ![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/8dfab179-b523-4561-ba04-c35bac9377cb) With an understanding of ERD line endings.A unary relationship is when an entity has a connection with itself.A binary relationship connects two entities, as seen in Figure 3.2. A ternary relationship connects three entities. For example, you might use a ticket entity to connect a venue, a performing artist, and a price.Binary relationships are the most common and easy to explore, whereas unary and ternary are comparatively complex and rare.
Relational Databases
Relational databases are pieces of software that let you make an operational system out of an ERD. You start with a relational model and create a physical design. Relational entities correspond to database tables, and entity attributes correspond to table columns. When creating a database table, the ordering of columns does not matter because you can specify the column order when retrieving data from a table. When an attribute becomes a column, you assign it a data type. Completing all of this work results in a diagram known as a schema. You can think of a schema as an ERD with the additional details needed to create a database. An associative table is both a table and a relationship. Recall that an animal can belong to more than one person, and a person can have more than one animal. An associative table lets you identify the relationship between a specific animal and a particular person with a minimum amount of data duplication. Let's examine the tables and their data in more detail to see the Animal, AnimalPerson, and Person tables in action.A primary key is one or more attributes that uniquely identify a specific row in a table. It is best to use a synthetic primary key, which is simply an attribute whose only purpose is to contain unique values for each row in the table. In Table 3.1, Animal_ID is a synthetic primary key, and the number 3 is arbitrarily assigned to Hazel. The number 3 has no meaning beyond its ability to uniquely identify a row. Nothing about Hazel's data changes if her Animal_ID was 7 instead of 3. Suppose both Hazel and Alexander belong to the Mangione family. There is nothing in the data that connects an animal to a person. To link the two tables, you need a foreign key. A foreign key is one or more columns in one table that points to corresponding columns in a related table. Frequently, a foreign key references another table's primary key. Looking at the AnimalPerson.Every row in a relational database must be unique. To populate the email template, you need the person's title, last name, and email address from Table 3.2 and the corresponding pet's name and animal type from Table 3.1. To pull data from a relational database table, you perform a query. You compose queries using a programming language called Structured Query Language (SQL).Foreign keys enforce referential integrity, or how consistent the data is in related tables. Consider Figure 3.9, which shows why referential integrity is crucial to enforcing data quality. Suppose you try to add a row to the middle table, specifying 6 for the Animal_ID and 99999 as the Person_ID. The foreign key on Animal_ID checks the table to the left to ensure a row exists with the Animal_ID of 6. The new record passes this check because there is a record with Animal_ID 6 for the cat named Alexander. When a similar check happens on Person_ID, there is no row corresponding to the Person_ID 99999 in the table on the right. Therefore, adding the new row fails, and the relationship between the tables is maintained.In Table 3.3, the Animal_ID column is a foreign key that points to the Animal_ID primary key in Table 3.1. Looking at the first data row in Table 3.3, Animal_ID 3 refers to Hazel. Similarly, the Person_ID column points to the Person_ID column in Table 3.2, with 10003 identifying Giacomo Mangione. The combination of both Animal_ID and Person_ID is what makes each row in Table 3.3 unique. Taken together, Animal_ID and Person_ID represent a composite primary key. As the name implies, a composite primary key is a primary key with more than one column.
Address Referential Integrity Example
Let's explore another example of how foreign keys enforce referential integrity. Suppose an organization has an order management system that stores customers' information and addresses, as shown in Figure 3.10.![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/2f847715-42bd-4696-8715-b6fd4646df39) With an understanding of cardinality, you can read that ERD in Figure 3.10 and see that:A specific customer can have many addresses, while a particular address belongs to a single customer.A specific address has a single address type, while a particular address type can apply to many different addresses.Implementing the relationships from an ERD enforces data constraints. For example, Figure 3.10 shows that an individual address must have a relationship with a state. You use foreign keys to implement data constraints in a database, like the relationship connecting Address with State. The FK designation for the State_Cd attribute on the Address entity in Figure 3.10 indicates that State_Cd is a foreign key. With a foreign key constraint in place, the database will not allow you to insert an address unless it exists in the State table, as shown in Figure 3.11.![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/8237d2c5-4792-4bf3-a012-35e800dd3c27)
Relational Database Providers
From a software standpoint, there are many relational database options. Oracle is one of the most mature database platforms and was first released in 1979. Over time, Microsoft developed SQL Server, and the open source community created offerings including MySQL, MariaDB, and PostgreSQL. Amazon Web Services (AWS) developed Aurora, which is compatible with MySQL and PostgreSQL. Aurora is unique because it takes advantage of AWS's underlying cloud platform and is easy to scale. Before we move onto Nonrelational Databases let's understand the differences between the two.
Nonrelati
A nonrelational database does not have a predefined structure based on tabular data. The result is a highly flexible approach to storing data. However, the data types available in relational databases are absent. As a result, you need to know more about the data itself to interact with it. Data validation happens in code, as opposed to being done in the database. 
A key-value database is one of the simplest ways of storing data. Data is stored as a collection of keys and their corresponding values. A key must be globally unique across the entire database. The use of keys differs from a relational database, where a given key identifies an individual row in a specific table. There are no structural limits on the values of a key. A key can be a sequence of numbers, alphanumeric strings, or some other combination of values. The data that corresponds with a key can be any structured or unstructured data type. Since there are no underlying table structures and few limitations on the data that can be stored, operating a key-value database is much simpler than a relational database.A document database is similar to a key-value database, with additional restrictions. In a key-value database, the value can contain anything. With a document database, the value is restricted to a specific structured format.Column-family databases use an index to identify data in groups of related columns. A relational database stores the data in a single table, where each row contains the Person_ID, Title, First_Name, Middle_Name, Last_Name, and Email columns. In a column-family database, the Person_ID becomes the index, while the other columns are stored independently. This design facilitates distributing data across multiple machines, which enables handling massive amounts of data. The ability to handle large data volumes is due to the technical implementation details of how these databases organize and store.Graph databases specialize in exploring relationships between pieces of data.
Database Use Cases.Different business needs require different database designs. While all databases store data, the database's structure needs to match its intended purpose. Business requirements impact the design of individual tables and how they are interconnected. Transactional and reporting systems need different implementation approaches to serve the people who use them efficiently. Databases tend to support two major categories of data processing: Online Transactional Processing (OLTP) and Online Analytical Processing (OLAP).
Online Transactional Processing
OLTP systems handle the transactions we encounter every day. Example transactions include booking a flight reservation, ordering something online, or executing a stock trade. While the number of transactions a system handles on a given day can be very high, individual transactions process small amounts of data. OLTP systems balance the ability to write and read data efficiently.
Normalization
Normalization is a process for structuring a database in a way that minimizes duplication of data.One of the principles is that a given piece of data is stored once and only once. As a result, a normalized database is ideal for processing transactions.
Online Analytical Processing
OLAP systems focus on the ability of organizations to analyze data. While OLAP and OLTP databases can both use relational database technology, their structures are fundamentally different. OLTP databases need to balance transactional read and write performance, resulting in a highly normalized design. Typically, OLTP databases are in 3NF.
Schema Concepts
The design of a database schema depends on the purpose it serves. Transactional systems require highly normalized databases, whereas a denormalized design is more appropriate for analytical systems. A data warehouse is a database that aggregates data from many transactional systems for analytical purposes. Transactional data may come from systems that power the human resources, sales, marketing, and product divisions. A data warehouse facilitates analytics across the entire company.
Data warehouses serve the entire organization, whereas data marts focus on the needs of a particular department within the organization. For example, suppose an organization wants to do analytics on their employees to understand retention and career evolution trends.
A data lake stores raw data in its native format instead of conforming to a relational database structure. Using a data lake is more complex than a data warehouse or data mart, as it requires additional knowledge about the raw data to make it analytically useful.
For data warehouses and data marts, several design patterns exist for modeling data. It is crucial to realize that the structure of a database schema impacts analytical efficiency, particularly as the volume of data grows. In addition to a schema's design, it is vital to consider the life cycle. Life-cycle considerations include where data comes from, how frequently it changes, and how long it needs to persist.
Star
The star schema design to facilitate analytical processing gets its name from what the schema looks like when looking at its entity relationship diagram.![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/e3dccc7a-a88c-494b-ade9-7476ccf8f37c)
Snowflake
Another design pattern for data warehousing is the snowflake schema. As its name implies, the schema diagram looks like a snowflake. Snowflake and star schemas are conceptually similar in that they both have a central fact table surrounded by dimensions. Where the approaches differ is in the handling of dimensions. With a star, the dimension tables connect directly to the fact table. With a snowflake, dimensions have subcategories, which gives the snowflake design its shape. A snowflake schema is less denormalized than the star schema.![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/b678d030-96b7-4c45-988a-212fee6e83d0)
Dimensionality
Dimensionality refers to the number of attributes a table has. The greater the number of attributes, the higher the dimensionality. A dimension table provides additional context around data in fact tables. For example, consider the Person_Dimension table in Figure 3.22, which contains details about people. If you need additional data about people, add columns to Person_Dimension.
Handling Dimensionality
There are multiple ways to design dimensions. Table 3.5 illustrates the start and end date approach. An understanding of this method is required to write a query to retrieve the current price. Another method extends the snowflake approach to modelling dimensions. You have a product dimension for the current price and a product history table for maintaining price history. One advantage of this approach is that it is easy to retrieve the current price while maintaining access to historical information.
Integration
Data from transactional systems flow into data warehouses and data marts for analysis. Recall that OLTP and OLAP databases have different internal structures. You need to retrieve, reshape, and insert data to move data between operational and analytical environments. 
One approach is known as extract, transform, and load (ETL). As the name implies, this method consists of three phases:
Extract:  In the first phase, you extract data from the source system and place it in a staging area. The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible.
Transform:  The second phase transforms the data. The goal is to reformat the data from its transactional structure to the data warehouse's analytical design.
Load:  The purpose of the load phase is to ensure data gets into the analytical system as quickly as possible.
ETL Vendors-Whether you choose ETL or ELT for loading your data warehouse, you don't have to write transformations by hand. Many products support both ETL and ELT. Before you pick one, carefully evaluate the available free and paid options to determine the one that best fits your needs and system architecture goals.
Data Collection Methods
Augmenting data from your transactional systems with external data is an excellent way to improve the analytical capabilities of your organization. For example, suppose you operate a national motorcycle rental fleet and want to determine if you need to rebalance your fleet across your existing locations. You also want to evaluate whether it is profitable to expand to a new geographic region, as well as predict the best time and place to add motorcycles to your fleet.
Application Programming Interfaces (APIs)
An application programming interface (API) is a structured method for computer systems to exchange information. APIs provide a consistent interface to calling applications, regardless of the internal database structure. Whoever calls an API has no idea whether a transactional or analytical data store backs it. The internal data structure does not matter as long as the API returns the data you want. APIs can be transactional, returning data as JSON objects. APIs can also facilitate bulk data extraction, returning CSV files.
Web Services
Many smartphone applications need a network connection, either cellular or Wi-Fi, to work correctly. The reason is that much of the data these applications need is not on the smartphone itself. Instead, data is found in private and public data sources and is accessible via a web service. A web service is an API you can call via Hypertext Transfer Protocol (HTTP), the language of the World Wide Web.
Web Scraping
Some of the data you want may not be available internally as an API or publicly via a web service. However, data may exist on a website. As seen in Chapter 2, data can present itself in an HTML table on a web page. If data exists in a structured format, you can retrieve it programmatically. Programmatic retrieval of data from a website is known as web scraping.
Human-in-the-Loop
There are times when the data you seek exists only in people's minds. For example, you can extract the most popular and profitable motorcycling destination from your existing internal data. You can get weather information from an API packaged as a web service. You can glean insight into competitive pricing by scraping your competitors' websites. Even with all of these data sources, you may still want insight into how customers feel about the services you provide.
Surveys
One way to collect data directly from your customers is by conducting a survey. The most simplistic surveys consist of one question and indicate customer satisfaction. For example, Figure 3.25 illustrates a survey collection approach in the airline industry. As people board their aircraft, they walk past a small machine with two buttons on it. In response to the question, the people press either the happy face or the unhappy face. Although single-question surveys don't provide any depth as to why people feel positively or negatively, they provide an overall indicator of satisfaction.
Survey Tools
Instead of designing a custom application to collect survey data, several survey products let you design complex surveys without worrying about building a database. Qualtrics is a powerful tool for developing and administering surveys. Figure 3.26 shows what it is like to build a survey in Qualtrics.
Observation
Observation is the act of collecting primary source data, from either people or machines. Observational data can be qualitative or quantitative. Collecting qualitative observational data leads to unstructured data challenges.
Sampling
Regardless of the data acquisition approach, you may end up with more data than is practical to manipulate. Imagine you are doing analytics in an Internet-of-Things environment, in which 800 billion events occur daily. Though it is possible, ingesting and storing 800 billion records is a challenging task. Manipulating 800 billion records takes a lot of computing power.
Data Manipulation
When manipulating data, one of four possible actions occurs:Create new data.Read existing data.Update existing data.Delete existing data.The acronym CRUD (Create, Read, Update, Delete) is a handy way to remember these four operations.
SQL Considerations
The keywords in SQL are case-insensitive. However, the case-sensitivity of column names and values depend on the database configuration.
Filtering
Examining a large table in its entirety provides insight into the overall population. To answer questions that an organization's leadership has typically requires a subset of the overall data. Filtering is a way to reduce the data down to only the rows that you need.
Filtering and Logical Operators
A query can have multiple filtering conditions. You need to use a logical operator to account for complex filtering needs. For example, suppose you need to retrieve the name and breed for dogs weighing more than 60 pounds. In that case, you can enhance the query using the AND logical operator,
Sorting
When querying a database, you frequently specify the order in which you want your results to return. The ORDER BY clause is the component of a SQL query that makes sorting possible. Similar to how the WHERE clause performs, you do not have to specify the columns you are using to sort the data in the SELECT clause.
Date Functions
Date columns also appear in transactional systems. Storing date information about an event facilitates analysis across time. For example, you may be interested in first-quarter sales performance or outstanding receivables on a rolling 60-day basis. Fortunately, there is an abundance of functions that make working with dates easy.
Logical Functions
Logical functions can make data substitutions when retrieving data. Remember that a SELECT statement only retrieves data. The data in the underlying tables do not change when a SELECT runs
Aggregate Functions
Summarized data helps answer questions that executives have, and aggregate functions are an easy way to summarize data. Aggregate functions summarize a query's data and return a single value. While each database platform supports different aggregation functions, 
System Functions
Each database platform offers functions that expose data about the database itself. One of the most frequently used system functions returns the current date. The current date is a component of transactional records and enables time-based analysis in the future. The current date is also necessary for a system that uses an effective date approach.
Query Optimization
Writing an SQL query is straightforward. Writing a SQL query that efficiently does what you intend can be more difficult. There are several factors to consider when creating well-performing SQL.
Chapter Summary
Databases are technology platforms for processing and storing data. There are two primary types of databases: relational and non-relational. Relational databases are ideal when you have tabular data, while there are numerous non-relational offerings when you need more flexibility than the structure a relational database imposes.Using a relational database as a technology platform, you can build transactional or analytical databases to address the business need. Transactional (OLTP) and analytical (OLAP) databases require different schema design approaches. Since a transactional database needs to balance reading and writing data, it follows a highly normalized schema design.On the other hand, analytical databases prioritize reading data and follow a denormalized approach. The star and snowflake schema designs are two approaches to structuring data for analysis. Both methods implement dimensional modeling, which organizes quantitative data into facts and qualitative data into dimensions.There are multiple ways to acquire data for analysis. For example, most data warehouses source data from transactional systems. To copy data from a transactional system, you can use an ETL or ELT approach. ETL leverages technology external to a relational database to transform data, while ELT uses the power of a relational database to do the transformation. Depending on the rate of change and data volume, you can take a complete refresh or delta load approach.You can also acquire data from external sources. APIs are integration components that encapsulate business logic and programmatically expose data. It is common to interact with APIs to source data for both transactional and analytical purposes. You may also find yourself needing to scrape data from a website or pull data from a public database.There are times when you need primary source data that you cannot obtain programmatically. In that case, you may end up conducting a survey or observing people and processes.Once you have data in a relational database, you need to be comfortable manipulating it. Structured Query Language (SQL) is the standard for relational data manipulation. With SQL queries, you can filter, sort, compare, and aggregate data.There are times when you will be working with large volumes of data that impact performance. There are several approaches you can take to mitigate performance issues. When writing frequently executed queries, make sure you use parametrization to reduce the database's parsing load. Reducing the number of records you're working with is another viable approach, which you can achieve by subsetting the data and using temporary tables. If you have queries taking more time than you expect, work with a database administrator to review the query's execution plan and ensure you have the appropriate indexing strategy.

# Chapter 4: Data Quality 
each data source has its own unique quality issues that need resolution before finding its way into a data warehouse. Whether designing an extract, transform, and load (ETL) process or digging into a new set of data warehouse tables, an analyst needs to examine each data source and resolve any underlying quality issues. Let's explore some common reasons for cleaning and profiling datasets.
Duplicate Data
Duplicate data occurs when data representing the same transaction is accidentally duplicated within a system. Suppose you want to open a spreadsheet on your local computer. To open the spreadsheet, you locate the file and double-click it. This method of opening documents establishes muscle memory that associates double-clicking with the desired action.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/51c0f843-329c-405c-8844-ff1929e4b6b3)
Redundant Data
While duplicate data typically comes from accidental data entry, redundant data happens when the same data elements exist in multiple places within a system. Frequently, data redundancy is a function of integrating multiple systems.For example, multiple source systems that perform different business functions and use shared data elements create the conditions for data redundancy. When a record changes in one system, there is no guarantee that its new value changes in another system. 
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/dbc01d28-1674-4f9c-ace3-f747b494804a)
Missing Values
Another issue that impacts data quality is the concept of missing values. Missing values occur when you expect an attribute to contain data but nothing is there. Missing values are also known as null values. A null value is the absence of a value. A null is not a space, blank, or other character. There are situations when allowing nulls makes sense. Suppose you are storing data about people and have a column for Middle Initial. Since not everyone has a middle initial, the Middle Initial column should be optional. When a column optionally contains data, it is nullable, meaning the column can contain null values. However, be aware that having nulls in a dataset poses calculation challenges.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/cea8a327-464d-4ff1-9992-215f2d5a02b4)
Invalid Data
Invalid data are values outside the valid range for a given attribute. An invalid value violates a business rule instead of having an incorrect data type. As such, you have to understand the context of a system to determine whether or not a value is invalid.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/e014d879-bd73-4e70-ad6a-03ff6e64f54c)
Nonparametric Data
Nonparametric data is data collected from categorical variables, which you read about in Chapter 2: Understanding Data. Sometimes the categories indicate differentiation, and sometimes they have a rank order associated with them. In this latter case, the rank order of the values is of significance, not the individual values themselves.For example, suppose a person has abdominal pain and seeks medical attention. The doctor asks the person to rate their pain on a scale of 0 to 10.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/d2abf254-0939-4a9d-9718-1d082de0dd77)
Data Outliers
A data outlier is a value that differs significantly from other observations in a dataset. Consider the real estate sale price example in Figure 4.10. All of the properties are on the same street, city, and state. Most of the properties have a sale price between $128,000 and $153,000. However, the property at 130 Main Street has a sale price of $26,496,400. That is a dramatic difference from the rest of the sales prices.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/99800325-9093-411e-9472-6e44b9daf9c4)
Specification Mismatch
A specification describes the target value for a component. A specification mismatch occurs when an individual component's characteristics are beyond the range of acceptable values. For example, suppose you want to add a room to a house and you buy 15 wooden studs. Looking at the blueprint for the addition, you need wooden studs with a rectangular cross-section measuring 2 inches by 4 inches (2×4). When purchasing the studs, you want to ensure that all 15 have a consistent cross-section.
Data Type Validation
Data type validation ensures that values in a dataset have a consistent data type. Consider the schema excerpt in Figure 4.11. The primary keys for both the Manufacturer and Model expect integer values, while the Manufacturer_Name and Model_Name are characters. Recall from Chapter 3 that the foreign key on Manufacturer_ID enforces referential integrity between the two tables.
![image](https://github.com/matthew23-stack/DataAnalytics/assets/127289268/b4b53107-0072-41ec-bcf2-692bbb09cbe6)

